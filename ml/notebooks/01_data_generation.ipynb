{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Memory Graph Data\n",
    "\n",
    "This notebook generates synthetic memory graphs for training the MemoryGNN model.\n",
    "\n",
    "We create realistic patient memory graphs at different Alzheimer's stages (0-7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch_geometric.data import Data\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Generate One Patient Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patient_memory_graph(num_memories=20, stage=2, patient_id=0):\n",
    "    \"\"\"\n",
    "    Generate a synthetic memory graph for one patient\n",
    "    \n",
    "    Node features (10 dimensions):\n",
    "    - type_one_hot (5): person, place, event, skill, routine\n",
    "    - recall_strength (1): 0-100 scaled to 0-1\n",
    "    - emotional_weight (1): 0-1\n",
    "    - importance (1): 1-10 scaled to 0-1\n",
    "    - age_days (1): how old the memory is\n",
    "    - access_freq (1): how often accessed\n",
    "    \"\"\"\n",
    "    memory_types = {'person': 0.35, 'place': 0.20, 'event': 0.20, 'skill': 0.15, 'routine': 0.10}\n",
    "    \n",
    "    node_features = []\n",
    "    initial_strengths = []\n",
    "    \n",
    "    for i in range(num_memories):\n",
    "        # One-hot encode memory type\n",
    "        type_probs = list(memory_types.values())\n",
    "        mem_type_idx = np.random.choice(len(type_probs), p=type_probs)\n",
    "        type_onehot = [0.0] * 5\n",
    "        type_onehot[mem_type_idx] = 1.0\n",
    "        \n",
    "        # Recall strength decreases with Alzheimer's stage\n",
    "        base_strength = np.random.uniform(60, 100) if stage <= 2 else np.random.uniform(30, 80)\n",
    "        stage_decay = stage * 8\n",
    "        recall_strength = max(10, base_strength - stage_decay + np.random.normal(0, 10))\n",
    "        \n",
    "        # Emotional memories are stronger\n",
    "        emotional_weight = np.random.beta(2, 5)\n",
    "        if emotional_weight > 0.8:\n",
    "            recall_strength += 10\n",
    "        \n",
    "        importance = np.random.randint(3, 11) / 10.0\n",
    "        age_days = np.random.exponential(scale=365 * 5) / (365 * 10)\n",
    "        access_freq = np.random.beta(2, 8)\n",
    "        \n",
    "        features = type_onehot + [\n",
    "            recall_strength / 100.0,\n",
    "            emotional_weight,\n",
    "            importance,\n",
    "            min(age_days, 1.0),\n",
    "            access_freq\n",
    "        ]\n",
    "        \n",
    "        node_features.append(features)\n",
    "        initial_strengths.append(recall_strength)\n",
    "    \n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Generate edges\n",
    "    edges = []\n",
    "    edge_weights = []\n",
    "    \n",
    "    for i in range(num_memories):\n",
    "        num_connections = np.random.randint(2, 6)\n",
    "        targets = np.random.choice(num_memories, size=num_connections, replace=False)\n",
    "        \n",
    "        for j in targets:\n",
    "            if i != j:\n",
    "                edges.append([i, j])\n",
    "                similarity = 1.0 - np.abs(node_features[i][5] - node_features[j][5])\n",
    "                edge_weights.append(similarity)\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_weights, dtype=torch.float).unsqueeze(1)\n",
    "    \n",
    "    # Target: memory decay predictions\n",
    "    y_node = []\n",
    "    for i, strength in enumerate(initial_strengths):\n",
    "        decay_rate = 0.01 * (stage + 1) + np.random.normal(0, 0.005)\n",
    "        \n",
    "        decay_30 = max(0, strength - (decay_rate * 30 * strength))\n",
    "        decay_90 = max(0, strength - (decay_rate * 90 * strength))\n",
    "        decay_180 = max(0, strength - (decay_rate * 180 * strength))\n",
    "        \n",
    "        if node_features[i][6] > 0.8:\n",
    "            decay_30 = min(100, decay_30 + 5)\n",
    "            decay_90 = min(100, decay_90 + 10)\n",
    "            decay_180 = min(100, decay_180 + 15)\n",
    "        \n",
    "        y_node.append([decay_30 / 100.0, decay_90 / 100.0, decay_180 / 100.0])\n",
    "    \n",
    "    y_node = torch.tensor(y_node, dtype=torch.float)\n",
    "    \n",
    "    # Graph-level risk score\n",
    "    avg_strength = np.mean(initial_strengths)\n",
    "    risk_score = 1.0 - (avg_strength / 100.0)\n",
    "    risk_score = min(1.0, max(0.0, risk_score + stage * 0.05))\n",
    "    y_graph = torch.tensor([risk_score], dtype=torch.float)\n",
    "    \n",
    "    data = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y_node=y_node,\n",
    "        y_graph=y_graph,\n",
    "        patient_id=patient_id,\n",
    "        stage=stage\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "print(\"✓ Function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Generate One Sample Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sample graph\n",
    "sample_graph = generate_patient_memory_graph(num_memories=15, stage=2, patient_id=0)\n",
    "\n",
    "print(f\"Number of nodes: {sample_graph.x.size(0)}\")\n",
    "print(f\"Number of edges: {sample_graph.edge_index.size(1)}\")\n",
    "print(f\"Node features shape: {sample_graph.x.shape}\")\n",
    "print(f\"Target shape (decay): {sample_graph.y_node.shape}\")\n",
    "print(f\"Risk score: {sample_graph.y_graph.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Full Dataset (1000 patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(num_patients=1000, output_dir='../data/synthetic'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    print(f\"Generating {num_patients} synthetic patient memory graphs...\")\n",
    "    \n",
    "    for i in range(num_patients):\n",
    "        # Random stage with realistic distribution\n",
    "        stage = np.random.choice([0, 1, 2, 3, 4, 5, 6, 7], \n",
    "                                p=[0.05, 0.15, 0.25, 0.25, 0.15, 0.08, 0.05, 0.02])\n",
    "        num_memories = np.random.randint(10, 41)\n",
    "        \n",
    "        data = generate_patient_memory_graph(num_memories=num_memories, stage=stage, patient_id=i)\n",
    "        \n",
    "        # 70-15-15 split\n",
    "        rand = np.random.rand()\n",
    "        if rand < 0.7:\n",
    "            train_data.append(data)\n",
    "        elif rand < 0.85:\n",
    "            val_data.append(data)\n",
    "        else:\n",
    "            test_data.append(data)\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Generated {i + 1}/{num_patients}\")\n",
    "    \n",
    "    # Save\n",
    "    with open(f'{output_dir}/train.pkl', 'wb') as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    with open(f'{output_dir}/val.pkl', 'wb') as f:\n",
    "        pickle.dump(val_data, f)\n",
    "    with open(f'{output_dir}/test.pkl', 'wb') as f:\n",
    "        pickle.dump(test_data, f)\n",
    "    \n",
    "    print(f\"\\nDataset saved to {output_dir}/\")\n",
    "    print(f\"Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)}\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Generate the dataset\n",
    "train_data, val_data, test_data = generate_dataset(num_patients=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = train_data + val_data + test_data\n",
    "\n",
    "# Extract statistics\n",
    "num_nodes = [data.x.size(0) for data in all_data]\n",
    "num_edges = [data.edge_index.size(1) for data in all_data]\n",
    "stages = [data.stage for data in all_data]\n",
    "risk_scores = [data.y_graph.item() for data in all_data]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Nodes distribution\n",
    "axes[0, 0].hist(num_nodes, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Distribution of Number of Memories per Patient')\n",
    "axes[0, 0].set_xlabel('Number of Memories')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Edges distribution\n",
    "axes[0, 1].hist(num_edges, bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[0, 1].set_title('Distribution of Memory Connections')\n",
    "axes[0, 1].set_xlabel('Number of Connections')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Stage distribution\n",
    "stage_counts = [stages.count(i) for i in range(8)]\n",
    "axes[1, 0].bar(range(8), stage_counts, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1, 0].set_title('Distribution of Alzheimer Stages')\n",
    "axes[1, 0].set_xlabel('Stage')\n",
    "axes[1, 0].set_ylabel('Number of Patients')\n",
    "axes[1, 0].set_xticks(range(8))\n",
    "\n",
    "# Risk score distribution\n",
    "axes[1, 1].hist(risk_scores, bins=30, edgecolor='black', alpha=0.7, color='red')\n",
    "axes[1, 1].set_title('Distribution of Risk Scores')\n",
    "axes[1, 1].set_xlabel('Risk Score (0-1)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"Average nodes per graph: {np.mean(num_nodes):.1f}\")\n",
    "print(f\"Average edges per graph: {np.mean(num_edges):.1f}\")\n",
    "print(f\"Average risk score: {np.mean(risk_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we have synthetic data, proceed to:\n",
    "- **Notebook 02**: Train MemoryGNN model\n",
    "- **Notebook 03**: Evaluate and visualize results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
