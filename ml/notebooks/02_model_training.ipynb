{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NeuroSmriti - Alzheimer's Detection Model Training\n",
        "\n",
        "This notebook trains machine learning models on the 420K+ synthetic dataset to predict Alzheimer's disease stages.\n",
        "\n",
        "## Models:\n",
        "1. Random Forest Classifier\n",
        "2. Gradient Boosting (XGBoost)\n",
        "3. Neural Network (MLP)\n",
        "4. Ensemble Voting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install pandas numpy scikit-learn xgboost matplotlib seaborn joblib tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "import os\n",
        "import joblib\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        ")\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    HAS_XGB = True\n",
        "    print(\"XGBoost available!\")\n",
        "except ImportError:\n",
        "    HAS_XGB = False\n",
        "    print(\"XGBoost not available, using GradientBoosting\")\n",
        "\n",
        "print(\"Libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data_path = '../data/alzheimers_420k_dataset.csv'\n",
        "\n",
        "if os.path.exists(data_path):\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(f\"Loaded {len(df):,} records from {data_path}\")\n",
        "else:\n",
        "    print(\"Dataset not found. Please run notebook 01_generate_dataset.ipynb first.\")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nColumn types:\")\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define feature columns\n",
        "feature_cols = [\n",
        "    'age', 'education_years', 'mmse_total', 'moca_total',\n",
        "    'hippocampus_volume', 'entorhinal_volume', 'total_brain_volume',\n",
        "    'csf_abeta42', 'csf_total_tau', 'csf_ptau181',\n",
        "    'amyloid_pet_suvr', 'tau_pet_suvr'\n",
        "]\n",
        "\n",
        "# Add binary features\n",
        "binary_cols = ['has_apoe4', 'family_history_ad', 'amyloid_positive', 'tau_positive',\n",
        "               'hypertension', 'diabetes', 'depression']\n",
        "\n",
        "# Convert boolean to int\n",
        "for col in binary_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].astype(int)\n",
        "\n",
        "# Add gender as binary\n",
        "df['gender_female'] = (df['gender'] == 'Female').astype(int)\n",
        "\n",
        "all_features = feature_cols + binary_cols + ['gender_female']\n",
        "available_features = [f for f in all_features if f in df.columns]\n",
        "\n",
        "print(f\"Using {len(available_features)} features:\")\n",
        "print(available_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X and y\n",
        "X = df[available_features].fillna(0).values\n",
        "y = df['diagnosis_stage'].values\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "print(f\"Classes: {label_encoder.classes_}\")\n",
        "print(f\"Encoded: {np.unique(y_encoded)}\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "for i, cls in enumerate(label_encoder.classes_):\n",
        "    count = np.sum(y_encoded == i)\n",
        "    print(f\"  {cls}: {count:,} ({count/len(y)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(f\"Feature matrix shape: {X_scaled.shape}\")\n",
        "print(f\"Feature means (should be ~0): {X_scaled.mean(axis=0)[:5]}\")\n",
        "print(f\"Feature stds (should be ~1): {X_scaled.std(axis=0)[:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(f\"Training set: {len(X_train):,} samples\")\n",
        "print(f\"Test set: {len(X_test):,} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=15,\n",
        "        min_samples_split=5,\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    ),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(\n",
        "        n_estimators=150,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42\n",
        "    ),\n",
        "    'Neural Network': MLPClassifier(\n",
        "        hidden_layer_sizes=(128, 64, 32),\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        max_iter=500,\n",
        "        early_stopping=True,\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "if HAS_XGB:\n",
        "    models['XGBoost'] = xgb.XGBClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=10,\n",
        "        learning_rate=0.1,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='mlogloss',\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "print(f\"Training {len(models)} models...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate each model\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training {name}...\")\n",
        "    print('='*50)\n",
        "    \n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    # Train\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    \n",
        "    # ROC AUC\n",
        "    if y_pred_proba is not None:\n",
        "        try:\n",
        "            roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')\n",
        "        except:\n",
        "            roc_auc = None\n",
        "    else:\n",
        "        roc_auc = None\n",
        "    \n",
        "    training_time = (datetime.now() - start_time).total_seconds()\n",
        "    \n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'model': model,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'training_time': training_time,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba\n",
        "    }\n",
        "    \n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:    {recall:.4f}\")\n",
        "    print(f\"  F1 Score:  {f1:.4f}\")\n",
        "    if roc_auc:\n",
        "        print(f\"  ROC AUC:   {roc_auc:.4f}\")\n",
        "    print(f\"  Time:      {training_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Ensemble Model\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Training Ensemble Model...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "estimators = [(name, results[name]['model']) for name in models.keys()]\n",
        "ensemble = VotingClassifier(estimators=estimators, voting='soft')\n",
        "\n",
        "start_time = datetime.now()\n",
        "ensemble.fit(X_train, y_train)\n",
        "\n",
        "y_pred_ensemble = ensemble.predict(X_test)\n",
        "accuracy_ensemble = accuracy_score(y_test, y_pred_ensemble)\n",
        "f1_ensemble = f1_score(y_test, y_pred_ensemble, average='weighted')\n",
        "training_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "results['Ensemble'] = {\n",
        "    'model': ensemble,\n",
        "    'accuracy': accuracy_ensemble,\n",
        "    'f1_score': f1_ensemble,\n",
        "    'training_time': training_time,\n",
        "    'y_pred': y_pred_ensemble\n",
        "}\n",
        "\n",
        "print(f\"  Accuracy: {accuracy_ensemble:.4f}\")\n",
        "print(f\"  F1 Score: {f1_ensemble:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all models\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': list(results.keys()),\n",
        "    'Accuracy': [r['accuracy'] for r in results.values()],\n",
        "    'F1 Score': [r['f1_score'] for r in results.values()],\n",
        "    'Training Time (s)': [r['training_time'] for r in results.values()]\n",
        "})\n",
        "\n",
        "comparison_df = comparison_df.sort_values('F1 Score', ascending=False)\n",
        "comparison_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy comparison\n",
        "ax1 = axes[0]\n",
        "colors = plt.cm.viridis(np.linspace(0, 0.8, len(results)))\n",
        "bars = ax1.bar(comparison_df['Model'], comparison_df['Accuracy'], color=colors)\n",
        "ax1.set_title('Model Accuracy Comparison', fontsize=14)\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_ylim(0.8, 1.0)\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Add value labels\n",
        "for bar, val in zip(bars, comparison_df['Accuracy']):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
        "             f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# F1 Score comparison\n",
        "ax2 = axes[1]\n",
        "bars = ax2.bar(comparison_df['Model'], comparison_df['F1 Score'], color=colors)\n",
        "ax2.set_title('Model F1 Score Comparison', fontsize=14)\n",
        "ax2.set_ylabel('F1 Score')\n",
        "ax2.set_ylim(0.8, 1.0)\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "for bar, val in zip(bars, comparison_df['F1 Score']):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
        "             f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../data/model_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Confusion Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrices for top models\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "for ax, (name, result) in zip(axes.flatten(), list(results.items())[:4]):\n",
        "    cm = confusion_matrix(y_test, result['y_pred'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "                xticklabels=label_encoder.classes_,\n",
        "                yticklabels=label_encoder.classes_)\n",
        "    ax.set_title(f'{name} Confusion Matrix')\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../data/confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importance from Random Forest\n",
        "rf_model = results['Random Forest']['model']\n",
        "importance = pd.DataFrame({\n",
        "    'Feature': available_features,\n",
        "    'Importance': rf_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "# Plot top features\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_n = 15\n",
        "colors = plt.cm.viridis(np.linspace(0.2, 0.8, top_n))\n",
        "plt.barh(importance['Feature'][:top_n][::-1], importance['Importance'][:top_n][::-1], color=colors)\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Top 15 Most Important Features (Random Forest)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../data/feature_importance.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Top 10 Features:\")\n",
        "print(importance.head(10).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5-fold cross-validation\n",
        "print(\"5-Fold Cross Validation Results:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "cv_results = {}\n",
        "for name in ['Random Forest', 'Gradient Boosting', 'Neural Network']:\n",
        "    model = results[name]['model']\n",
        "    scores = cross_val_score(model, X_scaled, y_encoded, cv=skf, scoring='accuracy', n_jobs=-1)\n",
        "    cv_results[name] = scores\n",
        "    print(f\"{name}: {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Boxplot of CV scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "cv_df = pd.DataFrame(cv_results)\n",
        "cv_df.boxplot()\n",
        "plt.title('5-Fold Cross-Validation Scores')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../data/cv_scores.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create models directory\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "\n",
        "# Save models\n",
        "for name, result in results.items():\n",
        "    filename = f\"../models/{name.lower().replace(' ', '_')}_model.pkl\"\n",
        "    joblib.dump(result['model'], filename)\n",
        "    print(f\"Saved: {filename}\")\n",
        "\n",
        "# Save scaler and label encoder\n",
        "joblib.dump(scaler, '../models/scaler.pkl')\n",
        "joblib.dump(label_encoder, '../models/label_encoder.pkl')\n",
        "\n",
        "# Save feature list\n",
        "import json\n",
        "with open('../models/feature_list.json', 'w') as f:\n",
        "    json.dump(available_features, f)\n",
        "\n",
        "print(\"\\nAll models and preprocessors saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save training report\n",
        "report = []\n",
        "report.append(\"=\"*70)\n",
        "report.append(\"NEUROSMRITI - ALZHEIMER'S DETECTION MODEL TRAINING REPORT\")\n",
        "report.append(\"=\"*70)\n",
        "report.append(f\"\\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "report.append(f\"Dataset Size: {len(df):,} records\")\n",
        "report.append(f\"Training Set: {len(X_train):,} samples\")\n",
        "report.append(f\"Test Set: {len(X_test):,} samples\")\n",
        "report.append(f\"Features: {len(available_features)}\")\n",
        "report.append(\"\\n\" + \"=\"*70)\n",
        "report.append(\"MODEL PERFORMANCE\")\n",
        "report.append(\"=\"*70)\n",
        "\n",
        "for name, result in results.items():\n",
        "    report.append(f\"\\n{name}:\")\n",
        "    report.append(f\"  Accuracy:  {result['accuracy']:.4f}\")\n",
        "    report.append(f\"  F1 Score:  {result['f1_score']:.4f}\")\n",
        "    report.append(f\"  Time:      {result['training_time']:.2f}s\")\n",
        "\n",
        "# Best model\n",
        "best_model = max(results.items(), key=lambda x: x[1]['f1_score'])\n",
        "report.append(\"\\n\" + \"=\"*70)\n",
        "report.append(f\"BEST MODEL: {best_model[0]}\")\n",
        "report.append(f\"F1 Score: {best_model[1]['f1_score']:.4f}\")\n",
        "report.append(\"=\"*70)\n",
        "\n",
        "report_text = \"\\n\".join(report)\n",
        "print(report_text)\n",
        "\n",
        "with open('../models/training_report.txt', 'w') as f:\n",
        "    f.write(report_text)\n",
        "\n",
        "print(\"\\nReport saved to ../models/training_report.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Training complete! Models saved to `../models/` directory."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
