{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NeuroSmriti - Model Testing & Evaluation\n",
        "\n",
        "This notebook tests the trained Alzheimer's detection models and provides comprehensive evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score, roc_curve,\n",
        "    precision_recall_curve, average_precision_score\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Models and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load models\n",
        "models = {}\n",
        "model_files = [\n",
        "    ('Random Forest', '../models/random_forest_model.pkl'),\n",
        "    ('Gradient Boosting', '../models/gradient_boosting_model.pkl'),\n",
        "    ('Neural Network', '../models/neural_network_model.pkl'),\n",
        "    ('Ensemble', '../models/ensemble_model.pkl')\n",
        "]\n",
        "\n",
        "for name, path in model_files:\n",
        "    if os.path.exists(path):\n",
        "        models[name] = joblib.load(path)\n",
        "        print(f\"Loaded: {name}\")\n",
        "    else:\n",
        "        print(f\"Not found: {path}\")\n",
        "\n",
        "# Load preprocessors\n",
        "scaler = joblib.load('../models/scaler.pkl')\n",
        "label_encoder = joblib.load('../models/label_encoder.pkl')\n",
        "\n",
        "with open('../models/feature_list.json', 'r') as f:\n",
        "    feature_list = json.load(f)\n",
        "\n",
        "print(f\"\\nLoaded {len(models)} models\")\n",
        "print(f\"Features: {len(feature_list)}\")\n",
        "print(f\"Classes: {label_encoder.classes_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test data\n",
        "df = pd.read_csv('../data/alzheimers_420k_dataset.csv')\n",
        "\n",
        "# Prepare features (same as training)\n",
        "binary_cols = ['has_apoe4', 'family_history_ad', 'amyloid_positive', 'tau_positive',\n",
        "               'hypertension', 'diabetes', 'depression']\n",
        "for col in binary_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].astype(int)\n",
        "\n",
        "df['gender_female'] = (df['gender'] == 'Female').astype(int)\n",
        "\n",
        "# Create test set (last 20%)\n",
        "test_size = int(len(df) * 0.2)\n",
        "test_df = df.tail(test_size)\n",
        "\n",
        "X_test = test_df[feature_list].fillna(0).values\n",
        "y_test = test_df['diagnosis_stage'].values\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "print(f\"Test set size: {len(X_test):,} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate each model\n",
        "evaluation_results = {}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"MODEL EVALUATION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n--- {name} ---\")\n",
        "    \n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled) if hasattr(model, 'predict_proba') else None\n",
        "    \n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test_encoded, y_pred)\n",
        "    precision = precision_score(y_test_encoded, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test_encoded, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test_encoded, y_pred, average='weighted')\n",
        "    \n",
        "    # Per-class metrics\n",
        "    precision_per_class = precision_score(y_test_encoded, y_pred, average=None)\n",
        "    recall_per_class = recall_score(y_test_encoded, y_pred, average=None)\n",
        "    f1_per_class = f1_score(y_test_encoded, y_pred, average=None)\n",
        "    \n",
        "    # ROC AUC\n",
        "    roc_auc = None\n",
        "    if y_pred_proba is not None:\n",
        "        try:\n",
        "            roc_auc = roc_auc_score(y_test_encoded, y_pred_proba, multi_class='ovr', average='weighted')\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    evaluation_results[name] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'precision_per_class': precision_per_class,\n",
        "        'recall_per_class': recall_per_class,\n",
        "        'f1_per_class': f1_per_class,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba\n",
        "    }\n",
        "    \n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 Score:  {f1:.4f}\")\n",
        "    if roc_auc:\n",
        "        print(f\"ROC AUC:   {roc_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed classification report for best model\n",
        "best_model_name = max(evaluation_results.keys(), key=lambda k: evaluation_results[k]['f1_score'])\n",
        "best_results = evaluation_results[best_model_name]\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"DETAILED REPORT: {best_model_name}\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "print(classification_report(y_test_encoded, best_results['y_pred'], target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Confusion Matrix Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed confusion matrix for best model\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Raw counts\n",
        "cm = confusion_matrix(y_test_encoded, best_results['y_pred'])\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "axes[0].set_title(f'{best_model_name} - Confusion Matrix (Counts)')\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('Actual')\n",
        "\n",
        "# Normalized\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=axes[1],\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "axes[1].set_title(f'{best_model_name} - Confusion Matrix (Normalized)')\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../data/test_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Misclassification analysis\n",
        "print(\"Misclassification Analysis:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    class_mask = y_test_encoded == i\n",
        "    class_total = class_mask.sum()\n",
        "    correct = (best_results['y_pred'][class_mask] == i).sum()\n",
        "    incorrect = class_total - correct\n",
        "    \n",
        "    print(f\"\\n{class_name.capitalize()}:\")\n",
        "    print(f\"  Total: {class_total:,}\")\n",
        "    print(f\"  Correct: {correct:,} ({correct/class_total*100:.1f}%)\")\n",
        "    print(f\"  Misclassified: {incorrect:,} ({incorrect/class_total*100:.1f}%)\")\n",
        "    \n",
        "    # Most common misclassifications\n",
        "    if incorrect > 0:\n",
        "        misclass = best_results['y_pred'][class_mask & (best_results['y_pred'] != i)]\n",
        "        if len(misclass) > 0:\n",
        "            most_common = np.bincount(misclass).argmax()\n",
        "            print(f\"  Most often confused with: {label_encoder.classes_[most_common]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Per-Class Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-class metrics comparison\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Class': label_encoder.classes_,\n",
        "    'Precision': best_results['precision_per_class'],\n",
        "    'Recall': best_results['recall_per_class'],\n",
        "    'F1 Score': best_results['f1_per_class']\n",
        "})\n",
        "\n",
        "print(\"Per-Class Metrics:\")\n",
        "print(metrics_df.to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "x = np.arange(len(label_encoder.classes_))\n",
        "width = 0.25\n",
        "\n",
        "ax.bar(x - width, metrics_df['Precision'], width, label='Precision', color='#3498db')\n",
        "ax.bar(x, metrics_df['Recall'], width, label='Recall', color='#2ecc71')\n",
        "ax.bar(x + width, metrics_df['F1 Score'], width, label='F1 Score', color='#e74c3c')\n",
        "\n",
        "ax.set_xlabel('Disease Stage')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Per-Class Performance Metrics')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([c.capitalize() for c in label_encoder.classes_])\n",
        "ax.legend()\n",
        "ax.set_ylim(0.8, 1.0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../data/per_class_metrics.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ROC Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROC curves for multi-class\n",
        "if best_results['y_pred_proba'] is not None:\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    \n",
        "    colors = plt.cm.viridis(np.linspace(0, 0.8, len(label_encoder.classes_)))\n",
        "    \n",
        "    for i, (class_name, color) in enumerate(zip(label_encoder.classes_, colors)):\n",
        "        # Binary labels for this class\n",
        "        y_binary = (y_test_encoded == i).astype(int)\n",
        "        y_score = best_results['y_pred_proba'][:, i]\n",
        "        \n",
        "        fpr, tpr, _ = roc_curve(y_binary, y_score)\n",
        "        auc = roc_auc_score(y_binary, y_score)\n",
        "        \n",
        "        ax.plot(fpr, tpr, color=color, lw=2, label=f'{class_name.capitalize()} (AUC = {auc:.3f})')\n",
        "    \n",
        "    ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.set_title(f'ROC Curves - {best_model_name}')\n",
        "    ax.legend(loc='lower right')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../data/roc_curves.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Probability predictions not available for ROC curves\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Prediction Confidence Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze prediction confidence\n",
        "if best_results['y_pred_proba'] is not None:\n",
        "    confidences = np.max(best_results['y_pred_proba'], axis=1)\n",
        "    \n",
        "    # Correct vs incorrect predictions\n",
        "    correct_mask = best_results['y_pred'] == y_test_encoded\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Confidence distribution\n",
        "    axes[0].hist(confidences[correct_mask], bins=50, alpha=0.7, label='Correct', color='#2ecc71')\n",
        "    axes[0].hist(confidences[~correct_mask], bins=50, alpha=0.7, label='Incorrect', color='#e74c3c')\n",
        "    axes[0].set_xlabel('Prediction Confidence')\n",
        "    axes[0].set_ylabel('Count')\n",
        "    axes[0].set_title('Prediction Confidence Distribution')\n",
        "    axes[0].legend()\n",
        "    \n",
        "    # Accuracy by confidence threshold\n",
        "    thresholds = np.linspace(0.5, 0.99, 20)\n",
        "    accuracies = []\n",
        "    coverages = []\n",
        "    \n",
        "    for thresh in thresholds:\n",
        "        mask = confidences >= thresh\n",
        "        if mask.sum() > 0:\n",
        "            acc = accuracy_score(y_test_encoded[mask], best_results['y_pred'][mask])\n",
        "            cov = mask.sum() / len(mask)\n",
        "            accuracies.append(acc)\n",
        "            coverages.append(cov)\n",
        "    \n",
        "    ax2 = axes[1]\n",
        "    ax2.plot(thresholds[:len(accuracies)], accuracies, 'b-', label='Accuracy', linewidth=2)\n",
        "    ax2.set_xlabel('Confidence Threshold')\n",
        "    ax2.set_ylabel('Accuracy', color='blue')\n",
        "    ax2.tick_params(axis='y', labelcolor='blue')\n",
        "    \n",
        "    ax3 = ax2.twinx()\n",
        "    ax3.plot(thresholds[:len(coverages)], coverages, 'r--', label='Coverage', linewidth=2)\n",
        "    ax3.set_ylabel('Coverage', color='red')\n",
        "    ax3.tick_params(axis='y', labelcolor='red')\n",
        "    \n",
        "    axes[1].set_title('Accuracy vs Coverage Trade-off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../data/confidence_analysis.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nConfidence Statistics:\")\n",
        "    print(f\"  Mean confidence: {confidences.mean():.3f}\")\n",
        "    print(f\"  Median confidence: {np.median(confidences):.3f}\")\n",
        "    print(f\"  Correct predictions avg confidence: {confidences[correct_mask].mean():.3f}\")\n",
        "    print(f\"  Incorrect predictions avg confidence: {confidences[~correct_mask].mean():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Sample Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show sample predictions\n",
        "print(\"Sample Predictions:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "sample_indices = np.random.choice(len(y_test), size=10, replace=False)\n",
        "\n",
        "for idx in sample_indices:\n",
        "    actual = label_encoder.classes_[y_test_encoded[idx]]\n",
        "    predicted = label_encoder.classes_[best_results['y_pred'][idx]]\n",
        "    correct = \"YES\" if actual == predicted else \"NO\"\n",
        "    \n",
        "    if best_results['y_pred_proba'] is not None:\n",
        "        confidence = best_results['y_pred_proba'][idx].max()\n",
        "        print(f\"Patient {idx}: Actual={actual:<10} Predicted={predicted:<10} Correct={correct:<4} Confidence={confidence:.2%}\")\n",
        "    else:\n",
        "        print(f\"Patient {idx}: Actual={actual:<10} Predicted={predicted:<10} Correct={correct}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Test Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save comprehensive test report\n",
        "test_report = {\n",
        "    \"test_date\": datetime.now().isoformat(),\n",
        "    \"test_size\": len(y_test),\n",
        "    \"models_tested\": list(models.keys()),\n",
        "    \"best_model\": best_model_name,\n",
        "    \"results\": {}\n",
        "}\n",
        "\n",
        "for name, results in evaluation_results.items():\n",
        "    test_report[\"results\"][name] = {\n",
        "        \"accuracy\": float(results['accuracy']),\n",
        "        \"precision\": float(results['precision']),\n",
        "        \"recall\": float(results['recall']),\n",
        "        \"f1_score\": float(results['f1_score']),\n",
        "        \"roc_auc\": float(results['roc_auc']) if results['roc_auc'] else None\n",
        "    }\n",
        "\n",
        "with open('../models/test_results.json', 'w') as f:\n",
        "    json.dump(test_report, f, indent=2)\n",
        "\n",
        "print(\"Test results saved to ../models/test_results.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL TEST SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTest Dataset Size: {len(y_test):,} samples\")\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  Accuracy:  {best_results['accuracy']:.4f} ({best_results['accuracy']*100:.2f}%)\")\n",
        "print(f\"  Precision: {best_results['precision']:.4f}\")\n",
        "print(f\"  Recall:    {best_results['recall']:.4f}\")\n",
        "print(f\"  F1 Score:  {best_results['f1_score']:.4f}\")\n",
        "if best_results['roc_auc']:\n",
        "    print(f\"  ROC AUC:   {best_results['roc_auc']:.4f}\")\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
